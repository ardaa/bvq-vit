\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{color}

\title{Vision Transformer (ViT) Architecture and Working Principle}
\author{}
\date{}

\begin{document}
\maketitle

\section{Overview}
The Vision Transformer (ViT) is a transformer-based architecture adapted for image classification. It processes images by dividing them into patches and treating them as a sequence of tokens, similar to how transformers process text.

\section{Image Processing Pipeline}

\subsection{Patch Creation and Embedding}
Given an input image $x \in \mathbb{R}^{H \times W \times C}$:
\begin{enumerate}
    \item Divide into $N$ patches of size $P \times P$
    \item $N = (H/P) \times (W/P)$ patches
    \item Each patch $x_p^i \in \mathbb{R}^{P^2C}$
\end{enumerate}

The embedding process:
\begin{equation}
    z_0 = [x_{class}; x_p^1E; x_p^2E; ...; x_p^NE] + E_{pos}
\end{equation}
where:
\begin{itemize}
    \item $E \in \mathbb{R}^{(P^2C) \times D}$: embedding matrix
    \item $E_{pos} \in \mathbb{R}^{(N+1) \times D}$: positional embeddings
    \item $x_{class}$: learnable classification token
\end{itemize}

\subsection{Transformer Encoder}
The embedded sequence passes through $L$ identical Transformer blocks. Each block consists of:

\subsubsection{Multi-Head Self-Attention (MSA)}
For each head $h$:
\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

The multi-head attention combines $h$ parallel attention heads:
\begin{equation}
    \text{MSA}(x) = [\text{head}_1; ...; \text{head}_h]W^O
\end{equation}

\subsubsection{MLP Block}
The MLP consists of two linear transformations with GELU activation:
\begin{equation}
    \text{MLP}(x) = \text{GELU}(xW_1 + b_1)W_2 + b_2
\end{equation}

\subsection{Complete Transformer Block}
Each block processes the input through:
\begin{enumerate}
    \item Layer Normalization and MSA:
    \begin{equation}
        z'_l = \text{MSA}(\text{LN}(z_{l-1})) + z_{l-1}
    \end{equation}
    
    \item Layer Normalization and MLP:
    \begin{equation}
        z_l = \text{MLP}(\text{LN}(z'_l)) + z'_l
    \end{equation}
\end{enumerate}

\section{Implementation Details}
The specific implementation uses:
\begin{itemize}
    \item Patch size: $P = 4$ (resulting in 64 patches for 32Ã—32 images)
    \item Embedding dimension: $D = 256$
    \item Number of attention heads: $h = 8$
    \item MLP hidden dimension: $4D = 1024$
    \item Number of Transformer blocks: $L = 6$
    \item Dropout rate: 0.1 for regularization
\end{itemize}

\section{Classification Process}
The final classification is performed using the [CLS] token:
\begin{equation}
    y = \text{MLP}(\text{LN}(z_L^0))
\end{equation}
where $z_L^0$ is the final representation of the [CLS] token after $L$ Transformer blocks.

\section{Key Advantages}
\begin{itemize}
    \item Global receptive field through self-attention
    \item Parallel processing of patches
    \item Scalable architecture
    \item Effective at capturing long-range dependencies
\end{itemize}

\end{document} 