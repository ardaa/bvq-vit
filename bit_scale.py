import matplotlib.pyplot as plt
import numpy as np

# Data: For each quantization level, the dictionary keys are layer names and 
# the values are tuples: (observed unique values, maximum expected values)
data = {
    "4-bit": {
        'patch_embed.proj': (16, 16),
        'blocks.0.attn.out_proj': (16, 16),
        'blocks.0.mlp.fc1': (16, 16),
        'blocks.0.mlp.fc2': (16, 16),
        'blocks.1.attn.out_proj': (16, 16),
        'blocks.1.mlp.fc1': (16, 16),
        'blocks.1.mlp.fc2': (16, 16),
        'blocks.2.attn.out_proj': (16, 16),
        'blocks.2.mlp.fc1': (16, 16),
        'blocks.2.mlp.fc2': (16, 16),
        'blocks.3.attn.out_proj': (16, 16),
        'blocks.3.mlp.fc1': (16, 16),
        'blocks.3.mlp.fc2': (16, 16),
        'blocks.4.attn.out_proj': (7, 16),
        'blocks.4.mlp.fc1': (16, 16),
        'blocks.4.mlp.fc2': (9, 16),
        'blocks.5.attn.out_proj': (16, 16),
        'blocks.5.mlp.fc1': (16, 16),
        'blocks.5.mlp.fc2': (16, 16),
        'blocks.6.attn.out_proj': (16, 16),
        'blocks.6.mlp.fc1': (16, 16),
        'blocks.6.mlp.fc2': (16, 16),
        'blocks.7.attn.out_proj': (16, 16),
        'blocks.7.mlp.fc1': (16, 16),
        'blocks.7.mlp.fc2': (16, 16),
        'blocks.8.attn.out_proj': (16, 16),
        'blocks.8.mlp.fc1': (7, 16),
        'blocks.8.mlp.fc2': (16, 16),
        'blocks.9.attn.out_proj': (16, 16),
        'blocks.9.mlp.fc1': (16, 16),
        'blocks.9.mlp.fc2': (16, 16),
        'blocks.10.attn.out_proj': (16, 16),
        'blocks.10.mlp.fc1': (16, 16),
        'blocks.10.mlp.fc2': (16, 16),
        'blocks.11.attn.out_proj': (16, 16),
        'blocks.11.mlp.fc1': (16, 16),
        'blocks.11.mlp.fc2': (16, 16),
        'head': (16, 16)
    },
    "6-bit": {
        'patch_embed.proj': (62, 64),
        'blocks.0.attn.out_proj': (58, 64),
        'blocks.0.mlp.fc1': (63, 64),
        'blocks.0.mlp.fc2': (63, 64),
        'blocks.1.attn.out_proj': (63, 64),
        'blocks.1.mlp.fc1': (54, 64),
        'blocks.1.mlp.fc2': (54, 64),
        'blocks.2.attn.out_proj': (61, 64),
        'blocks.2.mlp.fc1': (60, 64),
        'blocks.2.mlp.fc2': (62, 64),
        'blocks.3.attn.out_proj': (62, 64),
        'blocks.3.mlp.fc1': (63, 64),
        'blocks.3.mlp.fc2': (63, 64),
        'blocks.4.attn.out_proj': (23, 64),
        'blocks.4.mlp.fc1': (61, 64),
        'blocks.4.mlp.fc2': (33, 64),
        'blocks.5.attn.out_proj': (63, 64),
        'blocks.5.mlp.fc1': (60, 64),
        'blocks.5.mlp.fc2': (64, 64),
        'blocks.6.attn.out_proj': (62, 64),
        'blocks.6.mlp.fc1': (64, 64),
        'blocks.6.mlp.fc2': (63, 64),
        'blocks.7.attn.out_proj': (61, 64),
        'blocks.7.mlp.fc1': (60, 64),
        'blocks.7.mlp.fc2': (63, 64),
        'blocks.8.attn.out_proj': (59, 64),
        'blocks.8.mlp.fc1': (21, 64),
        'blocks.8.mlp.fc2': (63, 64),
        'blocks.9.attn.out_proj': (59, 64),
        'blocks.9.mlp.fc1': (63, 64),
        'blocks.9.mlp.fc2': (61, 64),
        'blocks.10.attn.out_proj': (58, 64),
        'blocks.10.mlp.fc1': (62, 64),
        'blocks.10.mlp.fc2': (61, 64),
        'blocks.11.attn.out_proj': (63, 64),
        'blocks.11.mlp.fc1': (60, 64),
        'blocks.11.mlp.fc2': (59, 64),
        'head': (64, 64),
    },
    "8-bit": {
        'patch_embed.proj': (236, 256),
        'blocks.0.attn.out_proj': (202, 256),
        'blocks.0.mlp.fc1': (235, 256),
        'blocks.0.mlp.fc2': (235, 256),
        'blocks.1.attn.out_proj': (230, 256),
        'blocks.1.mlp.fc1': (192, 256),
        'blocks.1.mlp.fc2': (167, 256),
        'blocks.2.attn.out_proj': (220, 256),
        'blocks.2.mlp.fc1': (226, 256),
        'blocks.2.mlp.fc2': (213, 256),
        'blocks.3.attn.out_proj': (238, 256),
        'blocks.3.mlp.fc1': (232, 256),
        'blocks.3.mlp.fc2': (222, 256),
        'blocks.4.attn.out_proj': (85, 256),
        'blocks.4.mlp.fc1': (226, 256),
        'blocks.4.mlp.fc2': (118, 256),
        'blocks.5.attn.out_proj': (234, 256),
        'blocks.5.mlp.fc1': (229, 256),
        'blocks.5.mlp.fc2': (222, 256),
        'blocks.6.attn.out_proj': (212, 256),
        'blocks.6.mlp.fc1': (246, 256),
        'blocks.6.mlp.fc2': (221, 256),
        'blocks.7.attn.out_proj': (225, 256),
        'blocks.7.mlp.fc1': (229, 256),
        'blocks.7.mlp.fc2': (231, 256),
        'blocks.8.attn.out_proj': (213, 256),
        'blocks.8.mlp.fc1': (74, 256),
        'blocks.8.mlp.fc2': (219, 256),
        'blocks.9.attn.out_proj': (211, 256),
        'blocks.9.mlp.fc1': (243, 256),
        'blocks.9.mlp.fc2': (206, 256),
        'blocks.10.attn.out_proj': (205, 256),
        'blocks.10.mlp.fc1': (235, 256),
        'blocks.10.mlp.fc2': (201, 256),
        'blocks.11.attn.out_proj': (231, 256),
        'blocks.11.mlp.fc1': (218, 256),
        'blocks.11.mlp.fc2': (192, 256),
        'head': (253, 256)
    },
    "10-bit": {
        'patch_embed.proj': (866, 1024),
        'blocks.0.attn.out_proj': (677, 1024),
        'blocks.0.mlp.fc1': (837, 1024),
        'blocks.0.mlp.fc2': (849, 1024),
        'blocks.1.attn.out_proj': (725, 1024),
        'blocks.1.mlp.fc1': (688, 1024),
        'blocks.1.mlp.fc2': (552, 1024),
        'blocks.2.attn.out_proj': (733, 1024),
        'blocks.2.mlp.fc1': (852, 1024),
        'blocks.2.mlp.fc2': (652, 1024),
        'blocks.3.attn.out_proj': (788, 1024),
        'blocks.3.mlp.fc1': (852, 1024),
        'blocks.3.mlp.fc2': (696, 1024),
        'blocks.4.attn.out_proj': (297, 1024),
        'blocks.4.mlp.fc1': (846, 1024),
        'blocks.4.mlp.fc2': (398, 1024),
        'blocks.5.attn.out_proj': (811, 1024),
        'blocks.5.mlp.fc1': (853, 1024),
        'blocks.5.mlp.fc2': (736, 1024),
        'blocks.6.attn.out_proj': (709, 1024),
        'blocks.6.mlp.fc1': (920, 1024),
        'blocks.6.mlp.fc2': (764, 1024),
        'blocks.7.attn.out_proj': (753, 1024),
        'blocks.7.mlp.fc1': (859, 1024),
        'blocks.7.mlp.fc2': (770, 1024),
        'blocks.8.attn.out_proj': (749, 1024),
        'blocks.8.mlp.fc1': (270, 1024),
        'blocks.8.mlp.fc2': (656, 1024),
        'blocks.9.attn.out_proj': (715, 1024),
        'blocks.9.mlp.fc1': (897, 1024),
        'blocks.9.mlp.fc2': (609, 1024),
        'blocks.10.attn.out_proj': (659, 1024),
        'blocks.10.mlp.fc1': (858, 1024),
        'blocks.10.mlp.fc2': (573, 1024),
        'blocks.11.attn.out_proj': (751, 1024),
        'blocks.11.mlp.fc1': (795, 1024),
        'blocks.11.mlp.fc2': (582, 1024),
        'head': (957, 1024)
    },
    "12-bit": {
        'patch_embed.proj': (3037, 4096),
        'blocks.0.attn.out_proj': (2224, 4096),
        'blocks.0.mlp.fc1': (2884, 4096),
        'blocks.0.mlp.fc2': (2897, 4096),
        'blocks.1.attn.out_proj': (2139, 4096),
        'blocks.1.mlp.fc1': (2457, 4096),
        'blocks.1.mlp.fc2': (1809, 4096),
        'blocks.2.attn.out_proj': (2140, 4096),
        'blocks.2.mlp.fc1': (3124, 4096),
        'blocks.2.mlp.fc2': (2036, 4096),
        'blocks.3.attn.out_proj': (2370, 4096),
        'blocks.3.mlp.fc1': (3122, 4096),
        'blocks.3.mlp.fc2': (2243, 4096),
        'blocks.4.attn.out_proj': (964, 4096),
        'blocks.4.mlp.fc1': (3121, 4096),
        'blocks.4.mlp.fc2': (1246, 4096),
        'blocks.5.attn.out_proj': (2686, 4096),
        'blocks.5.mlp.fc1': (3155, 4096),
        'blocks.5.mlp.fc2': (2422, 4096),
        'blocks.6.attn.out_proj': (2308, 4096),
        'blocks.6.mlp.fc1': (3406, 4096),
        'blocks.6.mlp.fc2': (2482, 4096),
        'blocks.7.attn.out_proj': (2543, 4096),
        'blocks.7.mlp.fc1': (3190, 4096),
        'blocks.7.mlp.fc2': (2514, 4096),
        'blocks.8.attn.out_proj': (2614, 4096),
        'blocks.8.mlp.fc1': (1003, 4096),
        'blocks.8.mlp.fc2': (2029, 4096),
        'blocks.9.attn.out_proj': (2439, 4096),
        'blocks.9.mlp.fc1': (3247, 4096),
        'blocks.9.mlp.fc2': (1828, 4096),
        'blocks.10.attn.out_proj': (2064, 4096),
        'blocks.10.mlp.fc1': (3097, 4096),
        'blocks.10.mlp.fc2': (1702, 4096),
        'blocks.11.attn.out_proj': (2110, 4096),
        'blocks.11.mlp.fc1': (2834, 4096),
        'blocks.11.mlp.fc2': (1913, 4096),
        'head': (3383, 4096)
    },
    "16-bit": {
        'patch_embed.proj': (32239, 65536),
        'blocks.0.attn.out_proj': (22459, 65536),
        'blocks.0.mlp.fc1': (31798, 65536),
        'blocks.0.mlp.fc2': (26195, 65536),
        'blocks.1.attn.out_proj': (19100, 65536),
        'blocks.1.mlp.fc1': (30994, 65536),
        'blocks.1.mlp.fc2': (20343, 65536),
        'blocks.2.attn.out_proj': (19714, 65536),
        'blocks.2.mlp.fc1': (40214, 65536),
        'blocks.2.mlp.fc2': (22156, 65536),
        'blocks.3.attn.out_proj': (22855, 65536),
        'blocks.3.mlp.fc1': (39922, 65536),
        'blocks.3.mlp.fc2': (25181, 65536),
        'blocks.4.attn.out_proj': (10339, 65536),
        'blocks.4.mlp.fc1': (40518, 65536),
        'blocks.4.mlp.fc2': (13637, 65536),
        'blocks.5.attn.out_proj': (28944, 65536),
        'blocks.5.mlp.fc1': (40652, 65536),
        'blocks.5.mlp.fc2': (26475, 65536),
        'blocks.6.attn.out_proj': (25425, 65536),
        'blocks.6.mlp.fc1': (43967, 65536),
        'blocks.6.mlp.fc2': (25763, 65536),
        'blocks.7.attn.out_proj': (28936, 65536),
        'blocks.7.mlp.fc1': (40803, 65536),
        'blocks.7.mlp.fc2': (26857, 65536),
        'blocks.8.attn.out_proj': (30450, 65536),
        'blocks.8.mlp.fc1': (13041, 65536),
        'blocks.8.mlp.fc2': (22503, 65536),
        'blocks.9.attn.out_proj': (28162, 65536),
        'blocks.9.mlp.fc1': (38960, 65536),
        'blocks.9.mlp.fc2': (19756, 65536),
        'blocks.10.attn.out_proj': (22092, 65536),
        'blocks.10.mlp.fc1': (36375, 65536),
        'blocks.10.mlp.fc2': (17923, 65536),
        'blocks.11.attn.out_proj': (16364, 65536),
        'blocks.11.mlp.fc1': (32845, 65536),
        'blocks.11.mlp.fc2': (20294, 65536),
        'head': (23537, 65536)
    }
}

# Create subplots: one row for each quantization level
num_levels = len(data)
fig, axs = plt.subplots(num_levels, 1, figsize=(14, 3 * num_levels), constrained_layout=True)

if num_levels == 1:
    axs = [axs]  # Ensure we always have a list of axes

for ax, (qbit, layers) in zip(axs, data.items()):
    layer_names = list(layers.keys())
    unique_vals = [layers[k][0] for k in layer_names]
    expected_vals = [layers[k][1] for k in layer_names]
    
    x = np.arange(len(layer_names))
    
    ax.bar(x, unique_vals, alpha=0.7, label='Observed Unique Values')
    # Optionally, you can plot the maximum expected as red markers for each layer.
    ax.plot(x, expected_vals, 'r--', marker='o', label='Max Expected')
    
    ax.set_title(f"{qbit} Quantization")
    ax.set_xlabel("Layer Name")
    ax.set_ylabel("Unique Value Count")
    ax.set_xticks(x)
    ax.set_xticklabels(layer_names, rotation=90, fontsize=8)
    ax.legend()
    ax.grid(True, linestyle='--', alpha=0.5)

plt.suptitle("Unique Values per Layer for Different Quantization Bit-Depths", fontsize=16)
plt.savefig("bit_scale.png")

plt.show()
